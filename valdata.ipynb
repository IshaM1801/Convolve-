{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Load the saved models\n",
    "rf_model = joblib.load('rf_model.pkl')\n",
    "lgb_model = joblib.load('lgb_model.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data=pd.read_csv('validation_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bureau_436           100.000000\n",
      "bureau_447           100.000000\n",
      "bureau_449            93.989280\n",
      "bureau_148            93.362366\n",
      "bureau_448            89.995693\n",
      "                        ...    \n",
      "onus_attribute_37      0.000000\n",
      "onus_attribute_38      0.000000\n",
      "onus_attribute_39      0.000000\n",
      "onus_attribute_40      0.000000\n",
      "account_number         0.000000\n",
      "Length: 1215, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "missing_percent = df.isnull().sum() / len(df) * 100\n",
    "print(missing_percent.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df = df.dropna(axis=1, how='all')\n",
    "columns_to_drop = df.columns[df.isna().all()]\n",
    "print(columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns with more than 70% missing values\n",
    "threshold = 0.7 * len(df)\n",
    "df = df.dropna(axis=1, thresh=threshold)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Median imputation for numerical columns\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "num_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "df[num_cols] = imputer.fit_transform(df[num_cols])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pickled pipeline\n",
    "with open('rf_model.pkl', 'rb') as file:\n",
    "    pipeline = pickle.load(file)\n",
    "\n",
    "# Get the list of expected feature names\n",
    "expected_columns = pipeline.named_steps['imputer'].feature_names_in_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pickled pipeline\n",
    "with open('lgb_model.pkl', 'rb') as file:\n",
    "    lgpipeline = pickle.load(file)\n",
    "\n",
    "# Get the list of expected feature names\n",
    "expected_columns = pipeline.named_steps['imputer'].feature_names_in_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure X_val matches the expected columns\n",
    "X_val = val_data[expected_columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure X_val matches the expected columns\n",
    "X_valgb = val_data[expected_columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add missing columns (if any) with default values\n",
    "for col in expected_columns:\n",
    "    if col not in X_val.columns:\n",
    "        X_val[col] = 0  # Or np.nan, depending on your imputer\n",
    "\n",
    "# Reorder columns to match expected order\n",
    "X_val = X_val[expected_columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add missing columns (if any) with default values\n",
    "for col in expected_columns:\n",
    "    if col not in X_val.columns:\n",
    "        X_valgb[col] = 0  # Or np.nan, depending on your imputer\n",
    "\n",
    "# Reorder columns to match expected order\n",
    "X_valgb = X_valgb[expected_columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pickled pipeline\n",
    "with open('rf_model.pkl', 'rb') as file:\n",
    "    pipeline = pickle.load(file)\n",
    "\n",
    "# Ensure validation data matches the training data structure\n",
    "expected_columns = pipeline.named_steps['imputer'].feature_names_in_\n",
    "X_val = val_data[expected_columns]  # Select and reorder columns\n",
    "for col in expected_columns:\n",
    "    if col not in X_val.columns:\n",
    "        X_val[col] = 0  # Add missing columns\n",
    "\n",
    "# Predict probabilities\n",
    "y_pred_prob = pipeline.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Save predictions\n",
    "predictions = pd.DataFrame({\n",
    "    'account_number': val_data['account_number'],  # Include account_number if available\n",
    "    'predicted_prob': y_pred_prob\n",
    "})\n",
    "predictions.to_csv('predictions.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ishamadlani/Library/Python/3.9/lib/python/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load pickled pipeline\n",
    "with open('lgb_model.pkl', 'rb') as file:\n",
    "    lgpipeline = pickle.load(file)\n",
    "\n",
    "# Ensure validation data matches the training data structure\n",
    "expected_columns = pipeline.named_steps['imputer'].feature_names_in_\n",
    "X_valgb = val_data[expected_columns]  # Select and reorder columns\n",
    "for col in expected_columns:\n",
    "    if col not in X_valgb.columns:\n",
    "        X_valgb[col] = 0  # Add missing columns\n",
    "\n",
    "# Predict probabilities\n",
    "y_pred_problg = lgpipeline.predict_proba(X_valgb)[:, 1]\n",
    "\n",
    "# Save predictions\n",
    "predictions = pd.DataFrame({\n",
    "    'account_number': val_data['account_number'],  # Include account_number if available\n",
    "    'predicted_prob': y_pred_problg\n",
    "})\n",
    "predictions.to_csv('lgbpredictions.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of predictions for class 0: 41600\n",
      "Number of predictions for class 1: 192\n"
     ]
    }
   ],
   "source": [
    "# Predict probabilities\n",
    "y_pred_prob = pipeline.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Convert probabilities to class predictions\n",
    "y_pred_class = (y_pred_prob >= 0.5).astype(int)  # Threshold of 0.5 for classification\n",
    "\n",
    "# Count predictions for each class\n",
    "count_zeros = (y_pred_class == 0).sum()\n",
    "count_ones = (y_pred_class == 1).sum()\n",
    "\n",
    "print(f\"Number of predictions for class 0: {count_zeros}\")\n",
    "print(f\"Number of predictions for class 1: {count_ones}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ishamadlani/Library/Python/3.9/lib/python/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of predictions in lgbm model for class 0: 41432\n",
      "Number of predictions in lgbm model for class 1: 360\n"
     ]
    }
   ],
   "source": [
    "# Predict probabilities\n",
    "y_pred_problg = pipeline.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Convert probabilities to class predictions\n",
    "y_pred_classlg = (y_pred_problg >= 0.5).astype(int)  # Threshold of 0.5 for classification\n",
    "\n",
    "# Count predictions for each class\n",
    "count_zeros = (y_pred_classlg == 0).sum()\n",
    "count_ones = (y_pred_classlg == 1).sum()\n",
    "\n",
    "print(f\"Number of predictions in lgbm model for class 0: {count_zeros}\")\n",
    "print(f\"Number of predictions in lgbm model for class 1: {count_ones}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   account_number  predicted_prob\n",
      "0  account_number             NaN\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load predictions\n",
    "predictions = pd.read_csv(\"predictions.csv\", header=None, names=[\"account_number\", \"predicted_prob\"])\n",
    "\n",
    "# Convert the 'predicted_prob' column to numeric, forcing errors to NaN\n",
    "predictions[\"predicted_prob\"] = pd.to_numeric(predictions[\"predicted_prob\"], errors='coerce')\n",
    "\n",
    "# Check if there are any NaN values (non-numeric entries)\n",
    "print(predictions[predictions[\"predicted_prob\"].isna()])\n",
    "\n",
    "# Optionally, drop rows with NaN in 'predicted_prob'\n",
    "predictions = predictions.dropna(subset=[\"predicted_prob\"])\n",
    "\n",
    "# Convert probabilities to binary predictions\n",
    "predictions[\"bad_flag\"] = (predictions[\"predicted_prob\"] >= 0.5).astype(int)\n",
    "\n",
    "# Save updated predictions\n",
    "predictions[[\"account_number\", \"bad_flag\"]].to_csv(\"final_predictions.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   account_number  predicted_prob\n",
      "0  account_number             NaN\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load predictions\n",
    "predictions = pd.read_csv(\"lgbpredictions.csv\", header=None, names=[\"account_number\", \"predicted_prob\"])\n",
    "\n",
    "# Convert the 'predicted_prob' column to numeric, forcing errors to NaN\n",
    "predictions[\"predicted_prob\"] = pd.to_numeric(predictions[\"predicted_prob\"], errors='coerce')\n",
    "\n",
    "# Check if there are any NaN values (non-numeric entries)\n",
    "print(predictions[predictions[\"predicted_prob\"].isna()])\n",
    "\n",
    "# Optionally, drop rows with NaN in 'predicted_prob'\n",
    "predictions = predictions.dropna(subset=[\"predicted_prob\"])\n",
    "\n",
    "# Convert probabilities to binary predictions\n",
    "predictions[\"bad_flag\"] = (predictions[\"predicted_prob\"] >= 0.5).astype(int)\n",
    "\n",
    "# Save updated predictions\n",
    "predictions[[\"account_number\", \"bad_flag\"]].to_csv(\"final_predictionslg.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RANDOM FOREST MODEL PREDICTIONS\n",
      "bad_flag\n",
      "0    41600\n",
      "1      192\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the final_predictions.csv file\n",
    "predictions = pd.read_csv(\"rf_final_predictions.csv\")\n",
    "\n",
    "# Count the occurrences of 0 and 1 in the 'bad_flag' column\n",
    "bad_flag_counts = predictions[\"bad_flag\"].value_counts()\n",
    "\n",
    "# Display the counts of 0 and 1\n",
    "print(\"RANDOM FOREST MODEL PREDICTIONS\")\n",
    "print(bad_flag_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM MODEL PREDICTIONS\n",
      "bad_flag\n",
      "0    41432\n",
      "1      360\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the final_predictions.csv file\n",
    "predictions = pd.read_csv(\"lgfinal_predictions.csv\")\n",
    "\n",
    "# Count the occurrences of 0 and 1 in the 'bad_flag' column\n",
    "bad_flag_counts = predictions[\"bad_flag\"].value_counts()\n",
    "\n",
    "# Display the counts of 0 and 1\n",
    "print(\"LGBM MODEL PREDICTIONS\")\n",
    "print(bad_flag_counts)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
